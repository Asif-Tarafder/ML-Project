# -*- coding: utf-8 -*-
"""Resume Screening Using machine learning

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1B9Fu8PBGVavso43zysf7NmJrDlGPgXOF

**Step 1:** Import the necessary library & data
"""

# importing the required libraries

import numpy as np
import pandas as pd
import seaborn as sns    #To visualize the data
import matplotlib.pyplot as plt
import re
# import time
#from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.multiclass import OneVsRestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics
from sklearn.metrics import accuracy_score,confusion_matrix
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB

from google.colab import drive
drive.mount('/content/drive')

# importing the data
data = pd.read_csv('/content/drive/MyDrive/csv folder for 422/UpdatedResumeDataSet.csv')
print("The number of rows are", data.shape[0],"\nThe number of columns are", data.shape[1])
data.head() #To see the column

# Checking the information of the dataframe(i.e the dataset)

#data.info()

# Checking all the different unique values

data.nunique()

"""**Step 2:** Look & vizualise the categories"""

#Print the categories


#data={'category':[...........],'resume':[......]}


print(data["Category"].unique())
#To see the total unique catagory
count = len(data["Category"].unique())
print(f"\nTotal unique catagory: {count}")

# See how much data in every category

print(data["Category"].value_counts())

#Now visualize the individual catagory

plt.figure(figsize = (15,15)) # Figure size
sns.countplot(y = "Category", data = data) #category will be on Y axis

"""**Step 3:** Calculate the distribution of each category & visualize it."""

# calculate the distribution of Categories

Category = data['Category'].value_counts().reset_index()['Category']
Labels = data['Category'].value_counts().reset_index()['index']

print(data["Category"].value_counts()*100/data.shape[0])   # data.shape[0] = total num of data

#Visualize the distribution of Categories

plt.figure(figsize = (15,15))
plt.title("Distribution of every category", fontsize=20)
plt.pie(Category, labels = Labels, autopct = '%1.2f%%', shadow = False)
print(data["Category"].value_counts()*100/data.shape[0])

"""**Step 4:** Pre-Processing the data"""

# Function to clean the data

def clean(data):
    data = re.sub('httpS+s*', ' ', data)       # Removing the links
    data = re.sub('RT|cc', ' ', data)           # Removing the RT and cc
    data = re.sub('#S+', ' ', data)             # Removing the hashtags
    data = re.sub('@S+', ' ', data)           # Removing the mentions
    data = data.lower()                      # Changing the test to lowercase
    data = ''.join([i if 32 < ord(i) < 128 else ' ' for i in data])    # Removing all the special characters
    data = re.sub('s+', 's', data)                                                      # Removing extra whitespaces
    data = re.sub('[%s]' % re.escape("""!"#$%&'()*+,-./:;<=>?@[]^_`{|}~"""), ' ', data)  # Removing punctuations
    return data


cleaned_data = data['Category'].to_frame()
cleaned_data['Resume'] = data['Resume'].apply(lambda x: clean(x))       # Applying the clean function
print(cleaned_data)

"""**Step 5:** Encoding the Category data & Creating a Word Vector using TfidfVectorizer"""

# Encoding the Category column using LabelEncoder

#encoder = LabelEncoder()
#cleaned_data['Category'] = encoder.fit_transform(cleaned_data['Category'])
#cleaned_data

# Encoded Classes

#encoder.classes_

# Creating a Word Vectorizer and transforming it

Resume = cleaned_data['Resume'].values
Category = cleaned_data['Category'].values
word_vectorizer = TfidfVectorizer(sublinear_tf = True, stop_words = 'english', max_features = 1000)
word_vectorizer.fit(Resume)
WordFeatures = word_vectorizer.transform(Resume)
data.head()

"""**Step 6:** Training our Machine Learning Model"""

# Splitting the data into train, test, printing the shape of each and running KNeighborsClassifier with OneVsRest method

X_train, X_test, y_train, y_test = train_test_split(WordFeatures, Category, random_state=2, test_size = 0.2)

#print(X_train)
# print(X_test)
print(y_train)
# print(y_train.shape)

print(f'The shape of the training data {X_train.shape[0]}')
print(f'The shape of the test data {X_test.shape[0]}')

"""Step 7 :KNeighbors Clasifier Modeling"""

# Predicting the values using the model built with train data and checking the appropriate metrics
# Computing the accuracy metrics and classification report
classified_data = OneVsRestClassifier(KNeighborsClassifier())
classified_data.fit(X_train, y_train)
k_pred = classified_data.predict(X_test)
print(f'Accuracy of KNeighbors Classifier on test set: {classified_data.score(X_test, y_test):.2f} out of 1.0 \n')
print(f'The classification report \n {metrics.classification_report(y_test, k_pred)}\n\n')

"""Step 7.1---Confusion matrix for KNeighbour"""

cm_kn = confusion_matrix(y_test, k_pred)
print(cm_kn)
#cm_df = pd.DataFrame(data=cm_kn,index = [..], columns = [..])
#but our data is in [[...],[...],[..]] form which complicates the situation that's why indexes and columns are ignored here

cm_matrix = pd.DataFrame(data=cm_kn)
plt.figure(figsize=(10,10))
sns.heatmap(cm_matrix, annot=True)
plt.title('Confusion Matrix for KNeighbour Accuracy')
plt.ylabel('Actal Values')
plt.xlabel('Predicted Values')
plt.show()

"""Step 8 : Support Vector Machine Classifier Modeling"""

svc_model = SVC()
svc_model.fit(X_train,y_train)
pred_svc = svc_model.predict(X_test)
print(f'Accuracy of Support Vector Machine model is : {accuracy_score(y_test,pred_svc):.2f} out of 1.0 and {accuracy_score(y_test,pred_svc) * 100}% in terms of percentage')
print(f'The classification report \n {metrics.classification_report(y_test, pred_svc)}\n\n')



#Another way to check if previous model was correct or not........

# from sklearn.preprocessing import LabelEncoder
# Encoder = LabelEncoder()
# Train_Y = Encoder.fit_transform(y_train)
# Test_Y = Encoder.fit_transform(y_test)

# from sklearn.feature_extraction.text import TfidfVectorizer
# from sklearn import model_selection, svm
# SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')
# SVM.fit(X_train,y_train)
# # // predict labels
# predictions_SVM = SVM.predict(X_test)
# # // get the accuracy
# print("Accuracy: ",accuracy_score(predictions_SVM, y_test)*100)

"""Step 8.1---Confusion matrix for Support Vector Model"""

cm_svc = confusion_matrix(y_test,pred_svc)
# Creating a dataframe for a array-formatted Confusion matrix,so it will be easy for plotting.
cm_df = pd.DataFrame(cm_svc)
#Plotting the confusion matrix
plt.figure(figsize=(10,10))
sns.heatmap(cm_df, annot=True)
plt.title('Confusion Matrix for SVC model accuracy')
plt.ylabel('Actal Values')
plt.xlabel('Predicted Values')
plt.show()

"""Step 9 -- Naive bayes Classifier Modeling"""

bayes = GaussianNB()
bayes.fit(X_train.toarray(), y_train)
bayes_pred = bayes.predict(X_test.toarray())

print(f'Accuracy of Naive Bayes model is : {accuracy_score(y_test,bayes_pred):.2f} out of 1.0 and {accuracy_score(y_test,bayes_pred) * 100}% in terms of percentage')
print(f'The classification report \n {metrics.classification_report(y_test, bayes_pred)}\n\n')

"""Step 9.1---Confusion Matrix for Naive Bayes model accuracy"""

cm_bayes = confusion_matrix(y_test,bayes_pred)
# Creating a dataframe for a array-formatted Confusion matrix,so it will be easy for plotting.
cm_df = pd.DataFrame(cm_bayes)
#Plotting the confusion matrix
plt.figure(figsize=(10,10))
sns.heatmap(cm_df, annot=True)
plt.title('Confusion Matrix for Naive Bayes')
plt.ylabel('Actal Values')
plt.xlabel('Predicted Values')
plt.show()